initialization:
    Iitialize wights and biases so that weights[l] stores the weight matrix for a given layer and biases[l] stores the bias vector for a given layer

forward propagation:
    For each layer, calculate the weighted sum and activation of all neurons and store it

gradient descent:
    For a certain number of iterations (training epochs):
        Calculate the cost
        Calculate the gradient using the cost
        Update the weights and biases using the gradient

backpropagation:
    Calculate the partial derivatives of the cost function with respect to the neuron activations of the last layer.
    Propagate the error through the network and calculate the partial derivatives of the cost function with respect to the neuron activations of all layers, starting from the last.
    With these results, calculate the partial derivatives of the cost function with respect to all weights and biases of the network - the gradient of the cost function with respect to weights and biases.
    Return the gradient.